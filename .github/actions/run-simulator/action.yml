name: "Run Simulator"
description: "Build BSPs for the given target/BSP and run tests on a simulator"

inputs:
  target:
    description: "The Arch/BSP to run. For example 'sparc/erc32'"
    type: string
    required: true
  sources-rtems:
    description: "Where to find the RTEMS sources. Use absolute path!"
    type: string
    required: true
  prefix:
    description: "Prefix directory with the installed tools. Use absolute path!"
    type: string
    required: true

outputs:
  passed_count:
    description: "Number of passed tests"
    value: ${{ steps.parse-result.outputs.passed_count }}
  failed_count:
    description: "Number of failed tests"
    value: ${{ steps.parse-result.outputs.failed_count }}
  user-input_count:
    description: "Number of tests with user input"
    value: ${{ steps.parse-result.outputs.user-input_count }}
  expected-fail_count:
    description: "Number of expected fails"
    value: ${{ steps.parse-result.outputs.expected-fail_count }}
  indeterminate_count:
    description: "Number of indeterminate tests"
    value: ${{ steps.parse-result.outputs.indeterminate_count }}
  benchmark_count:
    description: "Number of benchmark tests"
    value: ${{ steps.parse-result.outputs.benchmark_count }}
  timeout_count:
    description: "Number of tests that timed out"
    value: ${{ steps.parse-result.outputs.timeout_count }}
  test-too-long_count:
    description: "Number of too long tests"
    value: ${{ steps.parse-result.outputs.test-too-long_count }}
  wrong-version_count:
    description: "Number of tests with wrong version"
    value: ${{ steps.parse-result.outputs.wrong-version_count }}
  wrong-build_count:
    description: "Number of tests with wrong build"
    value: ${{ steps.parse-result.outputs.wrong-build_count }}
  wrong-tools_count:
    description: "Number of tests with wrong tools"
    value: ${{ steps.parse-result.outputs.wrong-tools_count }}
  wrong-header_count:
    description: "Number of tests with wrong header"
    value: ${{ steps.parse-result.outputs.wrong-header_count }}
  total_count:
    description: "Number of tests"
    value: ${{ steps.parse-result.outputs.total_count }}

runs:
  using: "composite"
  steps:
    - shell: bash
      run: |
        # Find correct parameters for this simulator
        args=""
        found=0
        [ "${{ inputs.target }}" == "sparc/erc32" ] && \
           args="--rtems-bsp=erc32-sis" && \
           found=1
        echo "rtems_test_args=${args}" >> $GITHUB_ENV
        echo "simulator_supported=${found}" >> $GITHUB_ENV
    - if: ${{ env.simulator_supported == 0 }}
      shell: bash
      run: |
        # Check whether parameters for this simulator have been found
        echo "${{ inputs.target }} is not supported for simulator runs"
        false
    - shell: bash
      run: |
        # create environment variables for paths and names
        shortname=`echo "${{ inputs.target }}" | sed -e "s|/|_|g"`
        echo "shortname=${shortname}" >> $GITHUB_ENV
        echo "logdir=${GITHUB_WORKSPACE}/logs-${shortname}" >> $GITHUB_ENV
        echo "builddir=${GITHUB_WORKSPACE}/build-${shortname}" >> $GITHUB_ENV
    - shell: bash
      run: |
        # create directories
        mkdir -p "${{ env.logdir }}"
        mkdir -p "${{ env.builddir }}"
    - shell: bash
      run: |
        # generate BSP config
        echo -e "[${{ inputs.target }}]\nBUILD_TESTS = True" >> "${{ env.builddir }}/config.ini"
        cat "${{ env.builddir }}/config.ini"
    - shell: bash
      run: |
        # configure
        cd ${{ env.builddir }} && ${{ inputs.sources-rtems }}/waf configure \
          --out="${{ env.builddir }}" \
          --rtems-config="${{ env.builddir }}/config.ini" \
          --top="${{ inputs.sources-rtems }}" \
          --prefix="${{ inputs.prefix }}"
    - if: ${{ failure() }}
      shell: bash
      run: |
        # print log
        cat "${{ env.builddir }}/config.log"
    - shell: bash
      run: |
        # compile
        cd ${{ env.builddir }} && ${{ inputs.sources-rtems }}/waf \
          --out="${{ env.builddir }}" \
          --top="${{ inputs.sources-rtems }}" \
          --prefix="${{ inputs.prefix }}"
        find ${{ env.builddir }} -name "*.exe"
    - shell: bash
      run: |
        # run tests
        ${{ inputs.prefix }}/bin/rtems-test \
          --log="${{ env.logdir }}/test.log" \
          --log-mode=all \
          --report-path="${{ env.logdir }}/report" \
          --report-format=json \
          --warn-all \
          --rtems-tools="${{ inputs.prefix }}" \
          ${{ env.rtems_test_args }} \
          ${{ env.builddir }}
    - name: Archive log
      uses: actions/upload-artifact@v3
      with:
        name: test-logs-${{ env.shortname }}
        path: logs-${{ env.shortname }}
    - id: parse-result
      shell: bash
      run: |
        # Generate github outputs from report
        jq -r '.["summary"]|to_entries[]|("::set-output name="+.key+"::"+(.value|tostring))' ${{ env.logdir }}/report.json
        echo "done"
    - if: ${{ steps.parse-result.outputs.failed_count != 0 || steps.parse-result.outputs.timeout_count != 0 }}
      shell: bash
      run: |
        # fail if tests failed
        echo "${{ steps.parse-result.outputs.failed_count }} tests failed."
        echo "${{ steps.parse-result.outputs.timeout_count }} tests timed out."
        false
