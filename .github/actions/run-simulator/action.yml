name: "Run Simulator"
description: "Build BSPs for the given target/BSP and run tests on a simulator"

inputs:
  target:
    description: "The Arch/BSP to run. For example 'sparc/erc32'"
    type: string
    required: true
  sources-rtems:
    description: "Where to find the RTEMS sources. Use absolute path!"
    type: string
    required: true
  prefix:
    description: "Prefix directory with the installed tools. Use absolute path!"
    type: string
    required: true

outputs:
  passed:
    description: "Number of passed tests"
    value: ${{ steps.parse-result.outputs.passed }
  failed:
    description: "Number of failed tests"
    value: ${{ steps.parse-result.outputs.failed }
  user-input:
    description: "Number of tests with user input"
    value: ${{ steps.parse-result.outputs.user-input }
  expected-fail:
    description: "Number of expected fails"
    value: ${{ steps.parse-result.outputs.expected-fail }
  indeterminate:
    description: "Number of indeterminate tests"
    value: ${{ steps.parse-result.outputs.indeterminate }
  benchmark:
    description: "Number of benchmark tests"
    value: ${{ steps.parse-result.outputs.benchmark }
  timeout:
    description: "Number of tests that timed out"
    value: ${{ steps.parse-result.outputs.timeout }
  test-too-long:
    description: "Number of too long tests"
    value: ${{ steps.parse-result.outputs.test-too-long }
  wrong-version:
    description: "Number of tests with wrong version"
    value: ${{ steps.parse-result.outputs.wrong-version }
  wrong-build:
    description: "Number of tests with wrong build"
    value: ${{ steps.parse-result.outputs.wrong-build }
  wrong-tools:
    description: "Number of tests with wrong tools"
    value: ${{ steps.parse-result.outputs.wrong-tools }
  wrong-header:
    description: "Number of tests with wrong header"
    value: ${{ steps.parse-result.outputs.wrong-header }
  total:
    description: "Number of tests"
    value: ${{ steps.parse-result.outputs.total }

runs:
  using: "composite"
  steps:
    - shell: bash
      run: |
        # Find correct parameters for this simulator
        args=""
        found=0
        [ "${{ inputs.target }}" == "sparc/erc32" ] && \
           args="--rtems-bsp=erc32-sis" && \
           found=1
        echo "rtems_test_args=${args}" >> $GITHUB_ENV
        echo "simulator_supported=${found}" >> $GITHUB_ENV
    - if: ${{ env.simulator_supported == 0 }}
      shell: bash
      run: |
        # Check whether parameters for this simulator have been found
        echo "${{ inputs.target }} is not supported for simulator runs"
        false
    - shell: bash
      run: |
        # create environment variables for paths and names
        shortname=`echo "${{ inputs.target }}" | sed -e "s|/|_|g"`
        echo "shortname=${shortname}" >> $GITHUB_ENV
        echo "logdir=${GITHUB_WORKSPACE}/logs-${shortname}" >> $GITHUB_ENV
        echo "builddir=${GITHUB_WORKSPACE}/build-${shortname}" >> $GITHUB_ENV
    - shell: bash
      run: |
        # create directories
        mkdir -p "${{ env.logdir }}"
        mkdir -p "${{ env.builddir }}"
    - shell: bash
      run: |
        # generate BSP config
        echo -e "[${{ inputs.target }}]\nBUILD_TESTS = True" >> "${{ env.builddir }}/config.ini"
        cat "${{ env.builddir }}/config.ini"
    - shell: bash
      run: |
        # configure
        cd ${{ env.builddir }} && ${{ inputs.sources-rtems }}/waf configure \
          --out="${{ env.builddir }}" \
          --rtems-config="${{ env.builddir }}/config.ini" \
          --top="${{ inputs.sources-rtems }}" \
          --prefix="${{ inputs.prefix }}"
    - if: ${{ failure() }}
      shell: bash
      run: |
        # print log
        cat "${{ env.builddir }}/config.log"
    - shell: bash
      run: |
        # compile
        cd ${{ env.builddir }} && ${{ inputs.sources-rtems }}/waf \
          --out="${{ env.builddir }}" \
          --top="${{ inputs.sources-rtems }}" \
          --prefix="${{ inputs.prefix }}"
        find ${{ env.builddir }} -name "*.exe"
    - shell: bash
      run: |
        # run tests
        ${{ inputs.prefix }}/bin/rtems-test \
          --log="${{ env.logdir }}/test.log" \
          --log-mode=all \
          --report-path="${{ env.logdir }}/report" \
          --report-format=json \
          --warn-all \
          --rtems-tools="${{ inputs.prefix }}" \
          ${{ env.rtems_test_args }} \
          ${{ env.builddir }}
    - name: Archive log
      uses: actions/upload-artifact@v3
      with:
        name: test-logs-${{ env.shortname }}
        path: logs-${{ env.shortname }}
    - id: parse-result
      shell: bash
      run: |
        json=${{ env.logdir }}/report.json
        echo "::set-output name=passed::`jq '.summary.passed_count' $json`"
        echo "::set-output name=failed::`jq '.summary.failed_count' $json`"
        echo "::set-output name=user-input::`jq '.summary.user-input_count' $json`"
        echo "::set-output name=expected-fail::`jq '.summary.expected-fail_count' $json`"
        echo "::set-output name=indeterminate::`jq '.summary.indeterminate_count' $json`"
        echo "::set-output name=benchmark::`jq '.summary.benchmark_count' $json`"
        echo "::set-output name=timeout::`jq '.summary.timeout_count' $json`"
        echo "::set-output name=test-too-long::`jq '.summary.test-too-long_count' $json`"
        echo "::set-output name=wrong-version::`jq '.summary.wrong-version_count' $json`"
        echo "::set-output name=wrong-build::`jq '.summary.wrong-build_count' $json`"
        echo "::set-output name=wrong-tools::`jq '.summary.wrong-tools_count' $json`"
        echo "::set-output name=wrong-header::`jq '.summary.wrong-header_count' $json`"
        echo "::set-output name=total::`jq '.summary.total_count' $json`"
    - if: ${{ steps.parse-result.outputs.failed != 0 || steps.parse-result.outputs.timeout != 0 }}
      shell: bash
      run: |
        # fail if tests failed
        false
